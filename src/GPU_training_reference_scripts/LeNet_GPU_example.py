"""LeNet_Keras_explained.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15EdM2cKnfks8Fe0JpSUlNaTxP6wnhTWN

### Practicing with Convolutional Deep Neural Networks: LeNet

In this notebook we are going to construct (in Keras) the LeNet architecture for classifying the images in the mnist data set. These images are 28x28 pixel ones depicting the number from 0 to 9. The LeNet network will be used to learn the number depicted in each image. 

![LeNet Architecture](https://drive.google.com/uc?export=view&id=1U2-j1W2zW7-SssIAMHRdaaXrQHgmN7ld)

Before going into further details of the architecture and dataset, we are going to import all the libraries we are going to use. As  you will notice, most of them are from the Keras Framework and Tensorflow.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D,Flatten, Dense
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import mnist
import tensorflow as tf
from tensorflow.keras.preprocessing import image


"""## Getting and Exploring the Dataset

We are loading the dataset using the Keras functionality. The dataset consits of 70,000 images. The function `mnist.load_data()` downloads the dataset and return two pairs: one containing data and labels for training and one containing data and training for validation.
"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# input image dimensions 28x28 pixel images. 
img_rows, img_cols = 28, 28

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

# one-hot encode the labels
# convert class vectors to binary class matrices
num_classes=10
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

"""We could explore the dataset by plotting some of the images and the associated label to it. Bare in mind this label should be the ground truth for each of the considered images. """

index = 42
print(y_train[index])

"""## Building the LeNet Architecture"""

model = Sequential()

"""## Convolutional Part
The convolutional part of the LeNet architecture interleaves convolutional layers and pooling layers. We can describe this part as follows:


1.   Convolutional layer of depth 6 with filters 5x5
2.   Pooling layer using the max operation for elements of size 2x2
3.   Convolutional layer of depth 16 with filters 5x5
4.   Pooling layer using the max operation for elements of size 2x2
5.   Convolutional layer of depth 120 with filters 5x5

Lets add all these layers using the keras interface. Convolutional layers can be contructed with the keras function ```Conv2D()``` and Pooling layers using the average operation ```AveragePooling2D()``` or the max operation ```MaxPooling2D()```. 




"""

# First Convolutional Layer (depth = 6 and filter size = 5x5)
model.add(Conv2D(filters=6, kernel_size=5, strides=1, activation='tanh', input_shape = (28,28,1), padding='same'))
# First Pooling layer
model.add(MaxPooling2D(pool_size=2,strides=2, padding='valid'))
# Second Convolutional Layer (depth = 16 filter size = 5x5)
model.add(Conv2D(filters=16,kernel_size=5,strides=1,activation='tanh',padding='valid'))
# Second Pooling layer
model.add(MaxPooling2D(pool_size=2,strides=2,padding='valid'))
# Third Convolutional Layer (depth = 120 and filter size= 5x5)
model.add(Conv2D(filters=120,kernel_size=5,strides=1,activation='tanh',padding='valid'))

"""The output of the convolutional part is a set of feature maps (i.e., the convoluted images resulting of convoluting the filters and images or feature maps from previous layers). These images are matrices of ```w``` channels. These matrices form the input for the upcoming fully connected layer. As the input of a NN is vector, the feature maps are flattened into a vector. The function ```Flatten()``` is in charge of this. """

# Next layer is a fully conneted NN, so we need flattering the output of the previous layer
model.add(Flatten())

# First Layer of the fully connected neural network for classification
model.add(Dense(units=84, activation='tanh'))
# Secon layer of the fully connected neurla network (the output of the whole network)
model.add(Dense(units=10,activation='softmax'))

"""Creating two layers of fully connected networks.

In Keras, the method ```sumary()``` of the model shows the output shape and number of parameters of each and every layer, as well as the whole network.
"""

model.summary()

"""Once the model has been created, we need to choose an optimizer in charge of minimizing the loss function and configure the model with an optimizer and a loss function. In this example, we are going to choose the Stochastic Gradient Descent and the categorical cross entropy loss function. """

opt = SGD()
model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])

"""After all the previous steps, we are already at the point where our model can be trained. For this, we will use the ```fit()``` function in Keras. This function requires to provide the training data as wellas the validation dataset. """

hist = model.fit(x_train, y_train, batch_size=32, epochs=3, validation_data=(x_test, y_test), verbose=2, shuffle=True)

